{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the seed page\n"
     ]
    }
   ],
   "source": [
    "import urllib\n",
    "max_limit=5\n",
    "\n",
    "def get_page(url):#This function is just to return the webpage contents; the source of the webpage when a url is given.\n",
    "    try:\n",
    "        f=urllib.urlopen(url)\n",
    "        page=f.read()\n",
    "        f.close()\n",
    "        return page\n",
    "    except:\n",
    "        return \"\"\n",
    "    return \"\"\n",
    "\n",
    "def union(a,b):#The union function merges the second list into first, with out duplicating an element of a, if it's already in a. Similar to set union operator. This function does not change b. If a=[1,2,3] b=[2,3,4]. After union(a,b) makes a=[1,2,3,4] and b=[2,3,4]\n",
    "    for e in b:\n",
    "        if e not in a:\n",
    "            a.append(e)\n",
    "\n",
    "\n",
    "def get_next_url(page):\n",
    "    start_link=page.find(\"a href\")\n",
    "    if start_link==-1:\n",
    "        return None,0\n",
    "    start_quote=page.find('\"',start_link)\n",
    "    end_quote=page.find('\"',start_quote+1)\n",
    "    url=page[start_quote+1:end_quote]\n",
    "    return url,end_quote\n",
    "     \n",
    "def get_all_links(page):\n",
    "    links=[]\n",
    "    while(True):\n",
    "        url,n=get_next_url(page)\n",
    "        page=page[n:]\n",
    "        if url:\n",
    "            links.append(url)\n",
    "        else:\n",
    "            break\n",
    "    return links\n",
    "    \n",
    "#The format of element in the index is <keyword>,[<List of urls that contain the keyword>]\n",
    "def add_to_index(index,url,keyword):\n",
    "    if keyword in index:\n",
    "        if url not in index[keyword]:\n",
    "            index[keyword].append(url)\n",
    "        return\n",
    "    index[keyword]=[url]\n",
    "\n",
    "def add_page_to_index(index,url,content):#Adding the content of the webpage to the index\n",
    "    for i in content.split():\n",
    "        add_to_index(index,url,i)\n",
    "        \n",
    "def Look_up(index,keyword):#This function is for given an index, it finds the keyword in the index and returns the list of links\n",
    "    if keyword in index:\n",
    "        return index[keyword]\n",
    "    return []\n",
    "\n",
    "def compute_ranks(graph):#Computing ranks for a given graph -> for all the links in it\n",
    "    d=0.8\n",
    "    numloops=10\n",
    "    ranks={}\n",
    "    npages=len(graph)\n",
    "    for page in graph:\n",
    "        ranks[page]=1.0/npages\n",
    "    for i in range(0,numloops):\n",
    "        newranks={}\n",
    "        for page in graph:\n",
    "            newrank=(1-d)/npages\n",
    "            for node in graph:\n",
    "                if page in graph[node]:\n",
    "                    newrank=newrank+d*ranks[node]/len(graph[node])  #PageRank equation  ????=?(?????)(?? ????/????)+(1???)1/??\n",
    "            newranks[page]=newrank\n",
    "        ranks=newranks\n",
    "    return ranks\n",
    "\n",
    "#Seed Page-This is the page,from which it starts crawling the web.The website to act as seed page is given as input\n",
    "def Crawl_web(seed):                                                \n",
    "    tocrawl=[seed] \n",
    "    crawled=[]\n",
    "    index={}\n",
    "    graph={} #new graph\n",
    "    global max_limit\n",
    "    while tocrawl:\n",
    "        p=tocrawl.pop()\n",
    "        if p not in crawled:\n",
    "            max_limit-=1\n",
    "            print(max_limit)\n",
    "            if max_limit<=0:\n",
    "                break\n",
    "            c=get_page(p)\n",
    "            add_page_to_index(index,p,c)   #linking url(p) to content of page(c) and storing in index\n",
    "            f=get_all_links(c)\n",
    "            union(tocrawl,f)\n",
    "            graph[p]=f\n",
    "            crawled.append(p)\n",
    "    return crawled,index,graph\n",
    "\n",
    "def QuickSort(pages,ranks):#Sorting in descending order\n",
    "    if len(pages)>1:\n",
    "        piv=ranks[pages[0]]\n",
    "        i=1\n",
    "        j=1\n",
    "        for j in range(1,len(pages)):\n",
    "            if ranks[pages[j]]>piv:\n",
    "                pages[i],pages[j]=pages[j],pages[i]\n",
    "                i+=1\n",
    "        pages[i-1],pages[0]=pages[0],pages[i-1]\n",
    "        QuickSort(pages[1:i],ranks)\n",
    "        QuickSort(pages[i+1:len(pages)],ranks)\n",
    "\n",
    "def Look_up_new(index,ranks,keyword):\n",
    "    pages=Look_up(index,keyword)\n",
    "    print('\\nPrinting the results as is with page rank\\n')\n",
    "    for i in pages:\n",
    "        print(i+\" --> \"+str(ranks[i])) #Displaying the lists, so that you can see the page rank along side\n",
    "    QuickSort(pages,ranks)\n",
    "    print(\"\\nAfter Sorting the results by page rank\\n\")\n",
    "    it=0\n",
    "    for i in pages:#This is how actually it looks like in search engine results - > sorted by page rank\n",
    "        it+=1\n",
    "        print(str(it)+'.\\t'+i+'\\n')\n",
    "\n",
    "\n",
    "\n",
    "print (\"Enter the seed page\")\n",
    "seed_page=input()\n",
    "print (\"Enter What you want to search\")\n",
    "search_term=input()\n",
    "try:\n",
    "    print (\"Enter the depth you wanna go\")\n",
    "    max_limit=int(input())\n",
    "except:\n",
    "    f=None\n",
    "print ('\\nStarted crawling, presently at depth..')\n",
    "crawled,index,graph=Crawl_web(seed_page)#printing all the links\n",
    "\n",
    "ranks=compute_ranks(graph)#Calculating the page ranks\n",
    "Look_up_new(index,ranks,search_term)\n",
    "\n",
    "#Seed page   http://xkcd.com/353\n",
    "#Enter What you want to search  is\n",
    "#depth 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
